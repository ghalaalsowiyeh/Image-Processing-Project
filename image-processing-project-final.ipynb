{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2819730,"sourceType":"datasetVersion","datasetId":1723812}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Pre-Proccessing \n\nimport os\nimport cv2\nimport numpy as np\nfrom skimage import io\n\ndef resize_image(image, target_size=(224, 224)):\n    \"\"\"Resize image to the desired target size using OpenCV.\"\"\"\n    return cv2.resize(image, target_size)\n\ndef gaussian_blur(image, ksize=(5, 5), sigma=1.5):\n    \"\"\"Apply Gaussian blur to reduce noise.\"\"\"\n    return cv2.GaussianBlur(image, ksize, sigma)\n\ndef preprocess_and_save_images(input_dir, output_dir, target_size=(224, 224)):\n    \"\"\"\n    Read all images from `input_dir`, resize, apply Gaussian blur, and \n    save them to `output_dir`.\n    \"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    \n    for image_name in os.listdir(input_dir):\n        image_path = os.path.join(input_dir, image_name)\n        \n        # Skip sub-directories or any non-image file\n        if os.path.isdir(image_path):\n            continue\n        \n        # Read the image (using skimage)\n        image = io.imread(image_path)\n        \n        # Resize + Gaussian blur\n        resized_image = resize_image(image, target_size)\n        blurred_image = gaussian_blur(resized_image)\n        \n        # Save preprocessed image\n        output_image_path = os.path.join(output_dir, image_name)\n        cv2.imwrite(output_image_path, blurred_image)\n        \n        print(f\"Processed and saved: {image_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:16:22.961854Z","iopub.execute_input":"2025-02-06T08:16:22.962070Z","iopub.status.idle":"2025-02-06T08:16:24.094149Z","shell.execute_reply.started":"2025-02-06T08:16:22.962048Z","shell.execute_reply":"2025-02-06T08:16:24.091585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# paths for dataset\ninput_dataset_path = \"/kaggle/input/messidor2preprocess/messidor-2/messidor-2/preprocess\"\noutput_preprocessed_path = \"/kaggle/working/messidor2_preprocessed\"\n\n# Preprocess images (resize + blur) and save\npreprocess_and_save_images(input_dataset_path, output_preprocessed_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:16:24.095688Z","iopub.execute_input":"2025-02-06T08:16:24.096162Z","iopub.status.idle":"2025-02-06T08:16:59.340645Z","shell.execute_reply.started":"2025-02-06T08:16:24.096131Z","shell.execute_reply":"2025-02-06T08:16:59.339827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Feature Extraction Techniques \n\nfrom skimage.feature import local_binary_pattern, hog\nfrom skimage import exposure\nimport matplotlib.pyplot as plt\n\ndef extract_hog_features(image):\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    features, hog_image = hog(gray_image, orientations=9, pixels_per_cell=(8, 8),\n                              cells_per_block=(2, 2), visualize=True)\n    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n    return hog_image_rescaled\n\ndef extract_lbp_features(image, radius=1, n_points=8):\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    lbp = local_binary_pattern(gray_image, n_points, radius, method='uniform')\n    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 59), range=(0, 58))\n    lbp_hist = lbp_hist.astype(float)\n    lbp_hist /= (lbp_hist.sum() + 1e-6)\n    return lbp_hist\n\ndef extract_sift_features(image):\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # SIFT in modern OpenCV (>=4.4.0) is cv2.SIFT_create()\n    sift = cv2.SIFT_create()\n    keypoints, descriptors = sift.detectAndCompute(gray_image, None)\n    img_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n    return img_with_keypoints\n\ndef plot_feature_extraction_results(original_image, hog_image, lbp_histogram, sift_image):\n    plt.figure(figsize=(12, 8))\n\n    # Original Image\n    plt.subplot(2, 2, 1)\n    plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for plotting\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # HOG\n    plt.subplot(2, 2, 2)\n    plt.imshow(hog_image, cmap='gray')\n    plt.title('HOG Features')\n    plt.axis('off')\n\n    # LBP Histogram\n    plt.subplot(2, 2, 3)\n    plt.plot(lbp_histogram)\n    plt.title('LBP Histogram')\n    plt.xlabel('LBP Value')\n    plt.ylabel('Frequency')\n\n    # SIFT\n    plt.subplot(2, 2, 4)\n    plt.imshow(cv2.cvtColor(sift_image, cv2.COLOR_BGR2RGB))\n    plt.title('SIFT Keypoints')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\ndef process_and_plot_image(image_path):\n    # Load the image with skimage\n    image = io.imread(image_path)\n    \n    # If the loaded image is RGBA or grayscale, you might need additional checks.\n    # Convert to BGR so that OpenCV works consistently:\n    if len(image.shape) == 2:  # Grayscale\n        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    elif image.shape[2] == 4:  # RGBA\n        image = cv2.cvtColor(image, cv2.COLOR_RGBA2BGR)\n        \n    # Extract features\n    hog_image = extract_hog_features(image)\n    lbp_histogram = extract_lbp_features(image)\n    sift_image = extract_sift_features(image)\n\n    # Plot\n    plot_feature_extraction_results(image, hog_image, lbp_histogram, sift_image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:16:59.341582Z","iopub.execute_input":"2025-02-06T08:16:59.341871Z","iopub.status.idle":"2025-02-06T08:16:59.370485Z","shell.execute_reply.started":"2025-02-06T08:16:59.341847Z","shell.execute_reply":"2025-02-06T08:16:59.369488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_image_name = \"/kaggle/working/messidor2_preprocessed/20051020_62878_0100_PP.png\"\nsample_image_path = os.path.join(output_preprocessed_path, sample_image_name)\n\n# Visualize\nprocess_and_plot_image(sample_image_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:16:59.372780Z","iopub.execute_input":"2025-02-06T08:16:59.373181Z","iopub.status.idle":"2025-02-06T08:17:00.331868Z","shell.execute_reply.started":"2025-02-06T08:16:59.373150Z","shell.execute_reply":"2025-02-06T08:17:00.330827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**1. Histogram of Oriented Gradients (HOG)**\n\n***Concept:*** \nHOG captures edge and gradient information from an image by computing the direction and magnitude of pixel gradients in local regions. The gradients are then binned into histograms, providing a robust descriptor of object shapes.\n\n***Applications:*** \nHOG can be used to detect abnormalities in medical images like retinal scans or X-rays. For instance, it can help detect irregular shapes or lesions in the retina for diabetic retinopathy detection.\n\n\n**2. Local Binary Patterns (LBP)**\n\n***Concept:*** LBP is a texture descriptor that compares each pixel with its neighbors. It encodes the result as a binary pattern and creates a histogram of these patterns to represent texture information.\n\n***Applications:*** LBP can be used for tumor classification in MRI scans or CT scans by distinguishing between healthy and abnormal tissue based on texture patterns, such as detecting subtle changes in the skin or brain tissue.\n\n\n\n\n**3. Scale-Invariant Feature Transform (SIFT)**\n\n***Concept:*** SIFT detects distinctive key points in images that are invariant to scale, rotation, and partially to illumination changes. Each key point is described by a feature vector based on local gradients.\n\n***Applications:*** SIFT can help in matching and aligning 3D scans (e.g., in CT or MRI) to detect tumors or lesions across different scans or timepoints, making it useful in monitoring disease progression.\n","metadata":{}},{"cell_type":"markdown","source":"# ***Sample Discussion***\n\n***1. HOG*** shows global edge orientation structure and highlights strong gradients around the disc and vessel edges.\n\n***2.LBP*** reveals dominant local textures, showing a strong bias toward smoother, uniform texture codes typical of retinal tissue.\n\n***3.SIFT*** pinpoints specific keypoints around strong local contrasts, especially near the optic disc and some vessel crossings.\n\n\n\n\n**These feature descriptors (HOG, LBP, SIFT) each capture different aspects of the image:**\n\n- HOG → edge/gradient patterns in a somewhat global sense.\n- LBP → local texture patterns, summarized in a histogram.\n- SIFT → distinct keypoints that can be matched or used for image alignment, or to describe local patches.","metadata":{}},{"cell_type":"code","source":"#ResNet Model\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\n\n# Define image transformations for ResNet\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Safe to ensure final size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ndef load_images(image_dir, transform):\n    \"\"\"\n    Loads all .jpg (or other) images from `image_dir`,\n    applies given transforms, and returns a single tensor plus file paths.\n    \"\"\"\n    images = []\n    image_paths = []\n    for img_name in os.listdir(image_dir):\n        # Adjust the condition if you have .png, .jpeg, etc.\n        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n            img_path = os.path.join(image_dir, img_name)\n            # Pillow handles reading\n            image = Image.open(img_path).convert(\"RGB\")\n            image = transform(image)\n            images.append(image)\n            image_paths.append(img_path)\n\n    if len(images) == 0:\n        raise ValueError(f\"No images found in {image_dir}\")\n\n    # Create a batch of shape [N, 3, 224, 224]\n    return torch.stack(images), image_paths\n\n# Load images from the preprocessed folder\nall_images, all_paths = load_images(output_preprocessed_path, transform)\n\n# Create a DataLoader\ndata_loader = DataLoader(all_images, batch_size=32, shuffle=False)\n\n# Load ResNet50 (pretrained)\nresnet_model = models.resnet50(pretrained=True)\n# Remove the final classification layer (FC) to get raw features of size 2048\nresnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])\nresnet_model.eval()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet_model = resnet_model.to(device)\n\n@torch.no_grad()\ndef extract_features(dataloader, model):\n    feature_list = []\n\n    for batch_images in dataloader:\n        batch_images = batch_images.to(device)\n        outputs = model(batch_images)               # shape: [batch_size, 2048, 1, 1]\n        outputs = outputs.view(outputs.size(0), -1) # shape: [batch_size, 2048]\n        feature_list.append(outputs.cpu())\n    \n    # Combine all features into one tensor of shape [N, 2048]\n    features = torch.cat(feature_list, dim=0)\n    return features\n\n# Extract features\nall_features = extract_features(data_loader, resnet_model)\n\nprint(\"Feature extraction completed!\")\nprint(f\"Extracted features shape: {all_features.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:17:00.332970Z","iopub.execute_input":"2025-02-06T08:17:00.333615Z","iopub.status.idle":"2025-02-06T08:17:18.133736Z","shell.execute_reply.started":"2025-02-06T08:17:00.333575Z","shell.execute_reply":"2025-02-06T08:17:18.132662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Advantages:**\n\n1. Transfer Learning Efficiency:\n\n\n* Pretrained models are already trained on large datasets. This allows them to learn useful and generalized features, which can be directly applied to related tasks.\n\n* By reusing pretrained models, you can avoid training a model from scratch, saving significant time.\n\n2. Improved Performance:\n\n* Since pretrained models are trained on large, diverse datasets, they often extract high-quality, robust features that lead to better performance on downstream tasks, even with small datasets.\n\n3. Reduced Data Requirements:\n\n* Training a model from scratch requires a large amount of labeled data. Pretrained models mitigate this by requiring only a smaller labeled dataset for fine-tuning or task-specific learning.\n\n\n**Limitations:**\n\n1. Domain Mismatch:\n\n* If the domain of the pretrained model's training data is significantly different from the target domain (e.g., a model trained on natural images applied to medical images), the extracted features may not be relevant or effective.\n\n\n2. Overfitting on Small Datasets:\n\n* When fine-tuning a pretrained model with a small dataset, there is a risk of overfitting, especially if the pretrained features are not sufficiently generalized for the target task.","metadata":{}},{"cell_type":"code","source":"#Image Enhancement\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Step 1: Define dataset paths\ninput_path = \"/kaggle/working/messidor2_preprocessed\"  # Original images (pre-processed images)\noutput_path = \"/kaggle/working/messidor2_enhanced\"  # Save enhanced images\n\n# Create output directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Step 2: Get all image file names\nimage_files = [f for f in os.listdir(input_path) if f.endswith(('.jpg', '.png'))]\nif len(image_files) == 0:\n    raise ValueError(\"No images found in the dataset path!\")\n\nprint(f\"Found {len(image_files)} images. Processing & saving enhanced versions...\")\n\n# Step 3: Process and Save Each Image\nfor i, image_file in enumerate(image_files):\n    image_path = os.path.join(input_path, image_file)\n    \n    # Load the image\n    original_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n\n    # Resize image for uniformity\n    resized_image = cv2.resize(original_image, (256, 256))\n\n    # 1. Apply Histogram Equalization (Grayscale)\n    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_RGB2GRAY)\n    equalized_image = cv2.equalizeHist(gray_image)\n\n    # 2. Apply Noise Reduction (Gaussian Blur)\n    noise_reduced_image = cv2.GaussianBlur(resized_image, (5, 5), 0)\n\n    # Save enhanced images\n    enhanced_image_path = os.path.join(output_path, image_file)\n\n    # Convert grayscale equalized image back to RGB for consistency\n    final_equalized = cv2.cvtColor(equalized_image, cv2.COLOR_GRAY2RGB)\n\n    # Save the noise reduced image (Final Enhanced Version)\n    cv2.imwrite(enhanced_image_path, cv2.cvtColor(noise_reduced_image, cv2.COLOR_RGB2BGR))  # Save in BGR format\n\n    # Show sample images\n    if i < 3:  # Show only the first 3 images\n        plt.figure(figsize=(12, 6))\n        \n        plt.subplot(1, 3, 1)\n        plt.title(\"Original\")\n        plt.imshow(resized_image)\n        plt.axis('off')\n\n        plt.subplot(1, 3, 2)\n        plt.title(\"Histogram Equalized\")\n        plt.imshow(final_equalized)\n        plt.axis('off')\n\n        plt.subplot(1, 3, 3)\n        plt.title(\"Noise Reduced\")\n        plt.imshow(noise_reduced_image)\n        plt.axis('off')\n\n        plt.show()\n\nprint(f\"Image enhancement completed. Enhanced images saved in: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:17:18.134888Z","iopub.execute_input":"2025-02-06T08:17:18.135452Z","iopub.status.idle":"2025-02-06T08:17:25.952867Z","shell.execute_reply.started":"2025-02-06T08:17:18.135410Z","shell.execute_reply":"2025-02-06T08:17:25.951829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Rational behind choosing enhancment**\n\n**1. Histogram Equalization**\n- Improves Contrast: Redistributes intensity levels to enhance visibility in underexposed or overexposed areas.\n  \n- Enhances Feature Visibility: Makes fine details more prominent, especially in subtle regions.\n\n  \n- Versatility: Effective for a wide range of applications, such as medical imaging or satellite data.\n\n  \n- Efficiency: Computationally simple and effective for real-time applications.\n\n  \n**2. Noise Reduction**\n- Removes Artifacts: Suppresses unwanted noise caused by environmental or sensor limitations.\n  \n- Improves Clarity: Produces cleaner, sharper images that are visually appealing and easier to analyze.\n\n  \n- Preserves Details: Advanced methods reduce noise without losing critical image information.","metadata":{}},{"cell_type":"markdown","source":"## Comparative Analysis\n### 1. Feature Analysis\nFeatures were extracted from the original images, images after histogram enhancement, and images after noise reduction. The histogram was used as a means to evaluate the distribution of pixel values in the images.\n### 2. Results\n- **Original Images**: The histogram shows the distribution of pixel values, with certain concentrations for shadows and highlights.\n- **Enhanced Images**: After applying histogram equalization, a better balance in the value distribution is observed, indicating an improvement in contrast.\n- **Noise Reduced Images**: The histogram for the color values after noise reduction shows a good balance, with a reduction in outlier values.\n### 3. Conclusion\nThe results indicate that histogram enhancement and noise reduction have a positive impact on image quality, which affects the extracted features. This improvement can be beneficial in applications such as classification or object detection.","metadata":{}},{"cell_type":"code","source":"# Classification Model\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n\n# 1) Helper Functions\ndef load_images_and_labels(image_dir, csv_file, target_size=(256, 256)):\n    \"\"\"\n    Loads images from `image_dir` using filenames from `csv_file`.\n    The CSV must have columns: 'id_code' and 'diagnosis'.\n    Returns:\n      - images (numpy array): shape = (num_samples, H, W, 3)\n      - labels (numpy array): shape = (num_samples,)\n    \"\"\"\n    df = pd.read_csv(csv_file)\n    \n    all_images = []\n    all_labels = []\n    \n    for _, row in df.iterrows():\n        # Image filename\n        img_name = row[\"id_code\"]  \n        # Label\n        label = row[\"diagnosis\"]  \n        \n        img_path = os.path.join(image_dir, img_name)\n        if not os.path.exists(img_path):\n            print(f\"Warning: {img_path} not found. Skipping.\")\n            continue\n        \n        # Load the image (BGR by default)\n        img = cv2.imread(img_path)\n        if img is None:\n            print(f\"Warning: Could not read {img_path}. Skipping.\")\n            continue\n        \n        # Resize to match the target size (256×256)\n        img = cv2.resize(img, target_size)\n        # Convert BGR -> RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        # Normalize to [0,1]\n        img = img.astype(np.float32) / 255.0\n        \n        all_images.append(img)\n        all_labels.append(label)\n    \n    return np.array(all_images), np.array(all_labels)\n\ndef flatten_data(X):\n    \"\"\"\n    Flatten image data from (N, H, W, C) to (N, H*W*C).\n    \"\"\"\n    return X.reshape(len(X), -1)\n\ndef build_simple_cnn(input_shape=(256,256,3), num_classes=1):\n    \"\"\"\n    Builds a simple CNN for binary classification.\n    If you have multi-class, adjust num_classes & activation accordingly.\n    \"\"\"\n    model = Sequential([\n        Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        MaxPooling2D((2,2)),\n        \n        Conv2D(64, (3,3), activation='relu'),\n        MaxPooling2D((2,2)),\n        \n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        \n        Dense(num_classes, activation='sigmoid')  # Binary classification\n    ])\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\ndef evaluate_metrics(y_true, y_pred, model_name, dataset_type, average='macro'):\n    \"\"\"\n    Evaluates accuracy, precision, recall, and F1-score for a model.\n    Handles both binary and multiclass classification.\n    Prints the results.\n    \n    Parameters:\n    - y_true: Ground truth labels\n    - y_pred: Predicted labels\n    - model_name: Name of the model (string)\n    - dataset_type: Type of dataset (e.g., \"Pre-processed\", \"Enhanced\")\n    - average: Averaging method for multiclass metrics ('micro', 'macro', 'weighted', or None)\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average=average)\n    recall = recall_score(y_true, y_pred, average=average)\n    f1 = f1_score(y_true, y_pred, average=average)\n    \n    print(f\"[{model_name}] Metrics ({dataset_type}):\")\n    print(f\"  Accuracy:  {accuracy:.3f}\")\n    print(f\"  Precision: {precision:.3f} (average='{average}')\")\n    print(f\"  Recall:    {recall:.3f} (average='{average}')\")\n    print(f\"  F1-Score:  {f1:.3f} (average='{average}')\")\n    \n    return accuracy, precision, recall, f1\n\n\n# 2) Paths & Data Loading\ncsv_file = \"/kaggle/input/messidor2preprocess/messidor_data.csv\"            # CSV file\npreprocessed_dir = \"/kaggle/working/messidor2_preprocessed\"  # directory for preprocessed images\nenhanced_dir = \"/kaggle/working/messidor2_enhanced\"         # directory for enhanced images\n\n# Load Pre-processed Images\nX_pre, y_pre = load_images_and_labels(\n    image_dir=preprocessed_dir,\n    csv_file=csv_file,\n    target_size=(256, 256)\n)\n\n# Load Enhanced Images\nX_enh, y_enh = load_images_and_labels(\n    image_dir=enhanced_dir,\n    csv_file=csv_file,\n    target_size=(256, 256)\n)\n\n# 3) Train-Test Split\nX_train_pre, X_test_pre, y_train_pre, y_test_pre = train_test_split(\n    X_pre, y_pre,\n    test_size=0.2,\n    stratify=y_pre,\n    random_state=42\n)\n\nX_train_enh, X_test_enh, y_train_enh, y_test_enh = train_test_split(\n    X_enh, y_enh,\n    test_size=0.2,\n    stratify=y_enh,\n    random_state=42\n)\n\n\n# 4A) Classification with SVM\n# Flatten images for classical ML\nX_train_pre_flat = flatten_data(X_train_pre)\nX_test_pre_flat  = flatten_data(X_test_pre)\nX_train_enh_flat = flatten_data(X_train_enh)\nX_test_enh_flat  = flatten_data(X_test_enh)\n\n# -- SVM on pre-processed images --\nsvm_pre = SVC(kernel='linear', random_state=42)\nsvm_pre.fit(X_train_pre_flat, y_train_pre)\ny_pred_pre_svm = svm_pre.predict(X_test_pre_flat)\nacc_pre_svm, prec_pre_svm, rec_pre_svm, f1_pre_svm = evaluate_metrics(\n    y_test_pre, y_pred_pre_svm, \"SVM\", \"Pre-processed\"\n)\n\n# -- SVM on enhanced images --\nsvm_enh = SVC(kernel='linear', random_state=42)\nsvm_enh.fit(X_train_enh_flat, y_train_enh)\ny_pred_enh_svm = svm_enh.predict(X_test_enh_flat)\nacc_enh_svm, prec_enh_svm, rec_enh_svm, f1_enh_svm = evaluate_metrics(\n    y_test_enh, y_pred_enh_svm, \"SVM\", \"Enhanced\"\n)\n\n\n# 4B) Classification with Random Forest\nrf_pre = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_pre.fit(X_train_pre_flat, y_train_pre)\ny_pred_pre_rf = rf_pre.predict(X_test_pre_flat)\nacc_pre_rf, prec_pre_rf, rec_pre_rf, f1_pre_rf = evaluate_metrics(\n    y_test_pre, y_pred_pre_rf, \"Random Forest\", \"Pre-processed\"\n)\n\nrf_enh = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_enh.fit(X_train_enh_flat, y_train_enh)\ny_pred_enh_rf = rf_enh.predict(X_test_enh_flat)\nacc_enh_rf, prec_enh_rf, rec_enh_rf, f1_enh_rf = evaluate_metrics(\n    y_test_enh, y_pred_enh_rf, \"Random Forest\", \"Enhanced\"\n)\n\n\n# 4C) Classification with CNN\n# -- CNN on pre-processed images --\ncnn_pre = build_simple_cnn(input_shape=(256,256,3), num_classes=1)  # binary classification\ncnn_pre.fit(\n    X_train_pre, y_train_pre,\n    epochs=5,\n    batch_size=16,\n    validation_split=0.2,\n    verbose=1\n)\ny_pred_pre_cnn = (cnn_pre.predict(X_test_pre) > 0.5).astype(int).reshape(-1)  # Predict and threshold binary outputs\nacc_pre_cnn, prec_pre_cnn, rec_pre_cnn, f1_pre_cnn = evaluate_metrics(\n    y_test_pre, y_pred_pre_cnn, \"CNN\", \"Pre-processed\"\n)\n\n# -- CNN on enhanced images --\ncnn_enh = build_simple_cnn(input_shape=(256,256,3), num_classes=1)\ncnn_enh.fit(\n    X_train_enh, y_train_enh,\n    epochs=5,\n    batch_size=16,\n    validation_split=0.2,\n    verbose=1\n)\ny_pred_enh_cnn = (cnn_enh.predict(X_test_enh) > 0.5).astype(int).reshape(-1)  # Predict and threshold binary outputs\nacc_enh_cnn, prec_enh_cnn, rec_enh_cnn, f1_enh_cnn = evaluate_metrics(\n    y_test_enh, y_pred_enh_cnn, \"CNN\", \"Enhanced\"\n)\n\n\n# 5) Print Summary of Results\nprint(\"\\n--- Final Results Comparison ---\")\nprint(\"SVM Metrics:\")\nprint(f\"  Pre-processed: Accuracy={acc_pre_svm:.3f}, Precision={prec_pre_svm:.3f}, Recall={rec_pre_svm:.3f}, F1-Score={f1_pre_svm:.3f}\")\nprint(f\"  Enhanced:      Accuracy={acc_enh_svm:.3f}, Precision={prec_enh_svm:.3f}, Recall={rec_enh_svm:.3f}, F1-Score={f1_enh_svm:.3f}\")\n\nprint(\"\\nRandom Forest Metrics:\")\nprint(f\"  Pre-processed: Accuracy={acc_pre_rf:.3f}, Precision={prec_pre_rf:.3f}, Recall={rec_pre_rf:.3f}, F1-Score={f1_pre_rf:.3f}\")\nprint(f\"  Enhanced:      Accuracy={acc_enh_rf:.3f}, Precision={prec_enh_rf:.3f}, Recall={rec_enh_rf:.3f}, F1-Score={f1_enh_rf:.3f}\")\n\nprint(\"\\nCNN Metrics:\")\nprint(f\"  Pre-processed: Accuracy={acc_pre_cnn:.3f}, Precision={prec_pre_cnn:.3f}, Recall={rec_pre_cnn:.3f}, F1-Score={f1_pre_cnn:.3f}\")\nprint(f\"  Enhanced:      Accuracy={acc_enh_cnn:.3f}, Precision={prec_enh_cnn:.3f}, Recall={rec_enh_cnn:.3f}, F1-Score={f1_enh_cnn:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T08:28:46.683913Z","iopub.execute_input":"2025-02-06T08:28:46.684360Z","iopub.status.idle":"2025-02-06T08:38:43.977984Z","shell.execute_reply.started":"2025-02-06T08:28:46.684326Z","shell.execute_reply":"2025-02-06T08:38:43.976747Z"}},"outputs":[],"execution_count":null}]}